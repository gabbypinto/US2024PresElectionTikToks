{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install contractions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/gabrielapinto/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import bigrams\n",
    "\n",
    "import contractions\n",
    "import re \n",
    "import string\n",
    "\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "english_stopwords = stopwords.words('english')\n",
    "print(english_stopwords)\n",
    "# english_stopwords.append()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "datasetfilePath=\"\" #file path to csv file \n",
    "df = pd.read_csv(datasetfilePath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After language classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose english transcripts\n",
    "df = df[df['lang'] == 'en']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## clean text\n",
    "\n",
    "#removes stopwords\n",
    "def remove_stopwords_and_punctuation(text):\n",
    "    #expand contractions\n",
    "    text = contractions.fix(text)\n",
    "    #extra spaces\n",
    "    text = ' '.join(text.split())\n",
    "    #convert to lowercase\n",
    "    text = text.lower()\n",
    "    #remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    #remove special characters\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text) \n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    #remove stopwords\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word.lower() not in english_stopwords]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "\n",
    "\n",
    "#remove stopwords from each transcripts\n",
    "clean_texts = []\n",
    "texts = df['voice_to_text'].tolist()\n",
    "for transcript in texts:\n",
    "\n",
    "    clean_text = remove_stopwords(transcript)\n",
    "    clean_texts.append(clean_text)\n",
    "\n",
    "\n",
    "df['clean_voice_to_text'] = clean_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## BIGRAMS\n",
    "\n",
    "\n",
    "# Generate bigrams for each cleaned text and combine them into one list\n",
    "bigram_list = []\n",
    "for text in clean_texts:\n",
    "    words = text.split()\n",
    "    bigram_list.extend([' '.join(bigram) for bigram in bigrams(words)])\n",
    "\n",
    "# Count the frequency of each bigram\n",
    "bigram_counts = Counter(bigram_list)\n",
    "\n",
    "# Generate the word cloud for bigrams\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(bigram_counts)\n",
    "\n",
    "# Display the word cloud\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.savefig(\"english_word_clouds.png\")\n",
    "plt.show()\n",
    "\n",
    "# Count the number of unique bigrams in the word cloud\n",
    "unique_bigrams_in_cloud = len(wordcloud.words_)\n",
    "print(f\"Number of unique bigrams in the word cloud: {unique_bigrams_in_cloud}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigrams in Spanish Transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFile=\"\" #file path to full data path\n",
    "df = pd.read_csv(dataFile)\n",
    "df = df[df['lang']=='es'] #grab all texts that are classified as Spanish\n",
    "texts = df['voice_to_text'].tolist()\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "spanish_stopwords = stopwords.words('spanish')\n",
    "\n",
    "#function that removes stopwords\n",
    "def remove_stopwords(text):\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word.lower() not in spanish_stopwords]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "clean_texts = []\n",
    "for transcript in texts:\n",
    "    clean_text = remove_stopwords(transcript)\n",
    "    clean_texts.append(clean_text)\n",
    "\n",
    "\n",
    "# Generate bigrams for each cleaned text and combine them into one list\n",
    "bigram_list = []\n",
    "for text in clean_texts:\n",
    "    words = text.split()\n",
    "    bigram_list.extend([' '.join(bigram) for bigram in bigrams(words)])\n",
    "\n",
    "# Count the frequency of each bigram\n",
    "bigram_counts = Counter(bigram_list)\n",
    "\n",
    "# Generate the word cloud for bigrams\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(bigram_counts)\n",
    "\n",
    "# Display the word cloud\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.savefig(\"spanish_word_cloud.png\")\n",
    "plt.show()\n",
    "\n",
    "# Count the number of unique bigrams in the word cloud\n",
    "unique_bigrams_in_cloud = len(wordcloud.words_)\n",
    "print(f\"Number of unique bigrams in the word cloud: {unique_bigrams_in_cloud}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
